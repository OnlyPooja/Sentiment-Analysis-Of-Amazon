#VADER uses a predefined sentiment lexicon or dictionary, which contains a large list of words and their associated sentiment scores.
#VADER first tokenizes the input text, breaking it down into individual words and punctuation marks.
# For each word in the text, VADER looks up the word in its sentiment lexicon. Each word in the lexicon has an associated sentiment score,
# which indicates how positive or negative that word is.
#


#importing libraries

import numpy as np
import pandas as pd
import nltk  # Natural Language Toolkit, is a Python library that is used for natural language processing (NLP) and text analysis
# VADER-> It calculates sentiment scores (positive, negative, neutral, and compound) to determine the sentiment or emotion expressed in a given text.

from nltk.sentiment.vader import SentimentIntensityAnalyzer #SentimentIntensityAnalyzer:It is a pre-trained model that can analyze a piece of text and provide sentiment scores, including:

#Positive Score: A score representing the degree of positive sentiment in the text.
#Negative Score: A score representing the degree of negative sentiment in the text.
#Neutral Score: A score representing the degree of neutrality in the text.
#Compound Score: A composite score that combines the above three scores to give an overall sentiment score

import plotly.offline as pyo
# use it to create various types of interactive plots and visualizations, such as scatter plots, bar charts, line charts,

import plotly.graph_objs as go
#provides classes and functions for creating and configuring various chart types,

import re      #regular expression
from textblob import TextBlob       #TextBlob is a convenient library for getting started with various NLP tasks, especially for those who are new to natural language processing
from wordcloud import WordCloud     #WordCloud is a library used for creating word clouds from text data.
import seaborn as sns               #Seaborn is particularly useful for creating complex visualizations
import matplotlib.pyplot as plt
import cufflinks as cf              # It simplifies the process of creating interactive charts



#Initializing Plotly:


cf.go_offline()          #you are configuring Cufflinks to work in offline mode, which means that the Plotly charts generated by Cufflinks will be displayed directly without internet connection

import plotly.graph_objs as go
from plotly.subplots import make_subplots

import warnings                      #The warnings module allows you to control how warning messages are displayed and handled in your Python programs.
warnings.filterwarnings("ignore")    #used to suppress all warning messages
warnings.warn("this will not show")
pd.set_option('display.max_columns', None)


#Reading Data:

df=pd.read_csv(r"C:\Users\Pooja Joshi\Desktop\Machine Learning\sentiment analysis\amazon.csv")


#Data Preprocessing:

#wilson_lower_bound-->it's a tool to provide a more realistic estimate of something when you don't have a lot of data, taking into account the uncertainty due to the small sample size.
# these two lines of code sort the DataFrame by a specific column and
# then remove a specific column from the DataFrame.


df=df.sort_values("wilson_lower_bound",ascending = False)
df.drop('Unnamed: 0', inplace=True, axis=1)
print(df.head())


#Missing Values Analysis:  that calculates and displays information about missing values in the DataFrame.


# this function helps you analyze and understand which columns in your DataFrame have missing data
# and the extent of that missing data in terms of count and percentage.
def missing_values_analysis(df):
    na_columns_ = [col for col in df.columns if df[col].isnull().sum() > 0]  # Use () to call the sum method
    n_miss = df[na_columns_].isnull().sum().sort_values(ascending=True)
    ratio_ = (df[na_columns_].isnull().sum() / df.shape[0] * 100).sort_values(ascending=True)
    missing_df = pd.concat([n_miss, np.round(ratio_, 2)], axis=1, keys=['Missing Values', 'Ratio'])
    missing_df = pd.DataFrame(missing_df)
    return missing_df


# Checking DataFrame Summary:provides a summary of the DataFrame, including shape, data types, missing values, duplicated values, and quantiles of numeric columns.


def check_dataframe(df,head=5, tail=5):
    print("SHAPE".center(82,"~"))            # This line prints a line of '~' characters centered in a width of 82 characters, creating a separator for the output.
    print('Rows: {}'.format(df.shape[0]))    # prints the number of rows in the DataFrame.
    print('Columns: {}'.format(df.shape[1])) # prints the number of columns in the DataFrame.
    print("TYPES".center(82,'~'))
    print(df.dtypes)                         #: This line prints the data types of each column in the DataFrame,
    print("".center(82,"~"))
    print(missing_values_analysis(df))
    print('DUPLICATED VALUES'.center(83),'~')
    print(df.duplicated().sum())            # prints the number of duplicated rows in the DataFrame
    print("Quantiles".center(82,'~'))
    #print(df.quantile([0, 0.0005, 0.5, 0.0095, 0.0099, 0.01]))
    numeric_columns = df.select_dtypes(include=[np.number])
    print(numeric_columns.quantile([0, 0.0005, 0.5, 0.0095, 0.0099, 0.01]))
check_dataframe(df)


#Checking Unique Classes:

#, this function helps you analyze and understand the diversity of unique values in each column of a DataFrame, with columns having the most unique classes listed at the top of the result

def check_class(dataframe):
    nunique_df = pd.DataFrame({'Variable': dataframe.columns,
                               'Number of Unique Classes': [dataframe[i].nunique() for i in dataframe.columns]})
    nunique_df = nunique_df.sort_values(by='Number of Unique Classes', ascending=False)
    nunique_df = nunique_df.reset_index(drop=True)
    return nunique_df

#Printing DataFrame Summary:
print(check_class(df))


#Defining Color Constraints:This line defines a list of color codes that will be used for plotting.
constraints = ['#B34D22','#EBE00C','#1FEB0C','#0C92EB','#EB0CD5']


#Categorical Variable Summary:categorical_variable_summary(df, 'overall')
#This line calls the categorical_variable_summary function to create countplots and pie charts for the 'overall' column.


#The purpose of this function is to create and display a summary of a categorical variable within the DataFrame using Plotly charts.
def categorical_variable_summary(df, column_name):
    fig = make_subplots(rows=1, cols=2,
                        subplot_titles=('Countplot', 'Percentage'),
                        specs=[[{"type": "xy"}, {"type": "domain"}]])
    fig.add_trace(go.Bar(y=df[column_name].value_counts().values.tolist(),
                         x=[f'{label} ({count})' for label, count in
                            zip(df[column_name].value_counts().index, df[column_name].value_counts())],
                         text=df[column_name].value_counts().values.tolist(),
                         textfont=dict(size=14),
                         name=column_name,
                         textposition='auto',
                         showlegend=False,
                         marker=dict(color=constraints,
                                     line=dict(color='#DBE6EC',
                                               width=1))),
                  row=1, col=1)
    fig.add_trace(go.Pie(labels=df[column_name].value_counts().keys(),
                         values=df[column_name].value_counts().values,
                         textfont=dict(size=18),
                         textposition='auto',
                         showlegend=False,
                         name=column_name,
                         marker=dict(colors=constraints)),
                  row=1, col=2)
    fig.update_layout(title={'text': column_name,
                             'y': 0.9,
                             'x': 0.5,
                             'xanchor': 'center',
                             'yanchor': 'top'},
                      template='plotly_white')


    html_file_path = f'{column_name}_plot.html'
    pyo.plot(fig, filename=html_file_path, auto_open=False)
    print(f"Plot saved as {html_file_path}")
    pyo.plot(fig,filename='overall_plot.html')

categorical_variable_summary(df,'overall')


#rext preprocessing on review text
print(df.reviewText.head())                  #This line prints the first five rows of the 'reviewText' column,
review_example = df.reviewText[2031]         #This line selects the text from the 'reviewText' column at index 2031 and assigns it to the variable review_example
print(review_example)
review_example = re.sub("[^a=zA-Z]",'',review_example)     #remove all characters from review_example that are not letters
print(review_example)
review_example = review_example.lower().split()            # to lowercase and then splits it into a list of words
print(review_example)
rt=lambda x: re.sub("[^a-zA-Z]",' ',str(x))                #eplace all non-alphabetical characters with spaces.
df["reviewText"] = df["reviewText"].map(rt)
df["reviewText"] = df["reviewText"].str.lower()
print(df.head())


#Sentiment Analysis Using VADER: VADER sentiment analysis tool to analyze the sentiment of text in the 'reviewText' column and assign sentiment labels ('Positive', 'Negative', or 'Neutral') to each row in the DataFrame.

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

for index, row in df.iterrows():

    #polarity_scores function returns a dictionary of sentiment scores.
    score = SentimentIntensityAnalyzer().polarity_scores(row['reviewText'])          #calculates sentiment scores (positive, negative, neutral, and compound) for the 'reviewText

    neg = score['neg']
    neu = score['neu']
    pos = score['pos']
    if neg > pos:
        df.loc[index, 'sentiment'] = 'Negative'
    elif pos > neg:
        df.loc[index, 'sentiment'] = 'Positive'
    else:
        df.loc[index, 'sentiment'] = 'Neutral'

df[df['sentiment']=='Positive'].sort_values("wilson_lower_bound",
                                           ascending = False).head(5)

#Creating Sentiment Plots:

#his function creates a side-by-side visualization of the count and percentage distribution of sentiment labels in the 'sentiment' column of the DataFrame
categorical_variable_summary(df,'sentiment')